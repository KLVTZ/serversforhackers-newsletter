---
draft: true
title: The Log Edition
topics: [All About Logs, Managing Logs with Logrotate]

---

## All About Logs

Logs are important to application developers. They give us detailed error information about our applications and the systems supporting them. They also can give us clues about what may go wrong in the future.

### Where To Find Them

Logs are almost always found in `/var/log`. Usually they are owned by a privileged user and group, such as `root` or `adm`. Even if you don't have access to a privileged user (a user who can use `sudo`), you can often - but not always - read these files.

Within the `/var/log` directory, you'll find system and software logs. For example, in Debian/Ubuntu systems, you'll find your Apache access and errors logs in `/var/log/apache2`. In other OS distributions, this may be `/var/log/httpd`. Nginx logs are usually found in `/var/log/nginx`.

Not all log files are in a subdirectory. For example, if you're using PHP-FPM, you might find logs in `/var/log/php5-fpm.log`. MySQL is often similarly found at `/var/log/mysql.log` and `/var/log/mysql.err`.

Lastly, logs are oftenn split between access (or events) and error logs. Apache, Nginx and MySQL all have separate error and access/use logs.

### Watching Logs With Tail

Since logs may give you more error information than the browser when developing a web application, you may find it appropriate to monitor a log file. This is especially true in production environments where error reporting is turned off - they will only be displayed within log files.

To monitor a log file, we can "[tail](http://unixhelp.ed.ac.uk/CGI/man-cgi?tail)" it. This will show the last *n* lines of the file as they are written to it.

If I wanted to watch the last 50 lines of my apache error log for errors, I would run:

    tail -n 30 -f /var/log/apache2/error.log

### Virtual Hosts and Logs

If you read the previous newsletter on Apache virtual hosts, you may have noticed that I setup a [unique access and error log for each virtual host](https://gist.github.com/fideloper/2710970#file-vhost-sh-L44-L50). This lets you track the logs of just one website, rather than having multiple website events combined into one log.

For a site **example.com**, the log files might be `/var/log/apache2/example.com-access.log` and `/var/log/apache2/exampe.com-error.log`.

### Analyzing Log Files

Here are some simple examples of examining some Apache (or Nginx!) log files.

Some of the command line tools available for things like are [`grep`](http://unixhelp.ed.ac.uk/CGI/man-cgi?grep), [`zgrep`](http://unixhelp.ed.ac.uk/CGI/man-cgi?zgrep+1), [`awk`](http://www.hcs.harvard.edu/~dholland/computers/awk.html), [`cut`](http://linux.die.net/man/1/cut), [`sed`](http://www.panix.com/~elflord/unix/sed.html), [`sort`](http://unixhelp.ed.ac.uk/CGI/man-cgi?sort), [`uniq`](http://unixhelp.ed.ac.uk/CGI/man-cgi?uniq), [`head`](http://unixhelp.ed.ac.uk/CGI/man-cgi?head) and others. For example here's [how to count 404's per request in a group of access logs](http://thingelstad.com/count-404-in-group-of-access-logs/). This works for both regular and compressed (gzipped) access logs.

For more command-line tricks:

* [Analyzing Apache log files](http://www.the-art-of-web.com/system/logs/)
* [Sed and Awk examples](https://www.adayinthelifeof.nl/2010/12/11/sed-awk-examples/)

### More Advanced Analysis and Tooling

As you might imagine, there are LOTS of tools available to analyze and store logs. This becomes especially important if you have a multiple-server environment, each of which may be generating it's own collection of logs.

In situations like this, a common strategy is to combine log files to a central storage. This usually comes with a bonus of being both searchable and more easily analyzed.

Here's a list of some available loging tools:

#### Self Install

* [Webalizer](http://www.webalizer.org/) - Good for single servers, and often found in hosting accounts, this is good for basic analyzing of web traffic from Apache logs
* [Fluentd](http://fluentd.org/) - Open source tool to collect events and logs (in JSON)
* [Graylog2](http://graylog2.org/) - This both collects logs AND analyzes them. It's a powerful tool, often used in conjunction with [Graphite](http://graphite.wikidot.com/) to create meaningful graphs based on log analysis.
	* Some information on [Graylog2 and PHP Monolog](http://jeremycook.ca/2012/10/02/turbocharging-your-logs/)
* [Splunk](http://www.splunk.com/) - Enterprisey

#### PaaS

* [Loggly](https://www.loggly.com/)
* [Airbrake.io](https://airbrake.io/)
* [Sumplogic](http://www.sumologic.com/) - Enterprisey
* [Papertrail](https://papertrailapp.com/)
* [Logentries](https://logentries.com/)
* [Splunk Storm](https://www.splunkstorm.com/) - SaaS for Splunk

#### Lastly

Here's an interesting idea on logging: [Fingers Crossed](http://zeroturnaround.com/rebellabs/attack-of-the-logs-consider-building-a-smarter-log-handler/) logging is the idea of only saving logs which are near errors. If logs do not appear near errors, they are (eventually) discarded.

Lastly, [Monolog](https://github.com/Seldaek/monolog) has a [ton of handlers](https://github.com/Seldaek/monolog/tree/master/src/Monolog/Handler) which may clue you into other services or ideas on how to handle logging.

---

## Managing Logs with Logrotate

So how are log files managed within a server? [Logrotate](https://fedorahosted.org/logrotate/) is usually how. This program can handle keeping log files from becoming too large. It can regularly compress or delete old log files. It can run scripts before or after rotating logs. This stops your servers disk drive (or allocated disk space) from exploding.

**Here is [more on Logrotate, including how to configure it](http://fideloper.com/ubuntu-prod-logrotate)**. Note the example on how to upload log files to Amazon S3 automatically when each log is rotated. This is a good solution to back up logs for later analysis, but not the most elegant.

Not explicitly stated is that Logrotate runs on a daily cron job. Usually you can find a the `logrotate` script run at `/etc/cron.daily/logrotate`. Many OSes come with scripts which can be run regularly, simply by being placed in the correct directory, such as `/etc/cron.hourly`, `/etc/cron.daily`, `/etc/cron.weely` and `/etc/cron.monthly`.

## Resources

* [Log Rotate](http://fideloper.com/ubuntu-prod-logrotate)
* [Log Resources for Distributed Environment](http://fideloper.com/web-app-load-balancing)
* [Digital Ocean on Logrotate for Ubuntu](https://www.digitalocean.com/community/articles/how-to-manage-log-files-with-logrotate-on-ubuntu-12-10)
* [Run Logrotate manually](http://stackoverflow.com/questions/2117771/is-it-possible-to-run-one-logrotate-check-manually)

## Most Importantly:
<iframe width="420" height="315" src="//www.youtube.com/embed/2C7mNr5WMjA" frameborder="0" allowfullscreen></iframe>