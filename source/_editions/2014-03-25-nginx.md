---
draft: true
title: Nginx Instead
topics: [Nginx as Frontman, Nginx as a Load Balancer]

---

## DONT ADD THIS IN: SPONSOR
Zack at z19r.com.

## Nginx as Frontman

> <em>"Apache is like Microsoft Word, it has a million options but you only need six. Nginx does those six things, and it does five of them 50 times faster than Apache."</em> - [Chris Lea](http://wiki.nginx.org/WhyUseIt)

Nginx is a lightweight alternative to Apache. It does less things, but still covers 90% of the most common use cases.

When we run PHP with Apache, a typical setup is to use the PHP5 Module. This means PHP is essentially loaded and available on every request, even for static files and images! That creates some overhead for each web request.

Nginx, on the other hand, deals primarily with static files, prefering to hand-off other requests to another process. This way, Nginx doesn't have to load in PHP or know about Python or Ruby in order to handle a web request.

How, then, does Nginx hand-off a request to a dynamic application? As mentioned, it passes the request off to another process instead of handling the request itself. With PHP, for example, we would install PHP5-FPM alongside Nginx. Nginx would then hande-off any PHP request to PHP5-FPM to process and respond to. In fact, we can install any type of CGI/FastCGI process, or even another http server, to handle a request. These would then process the request and return the result for Nginx to ultimately return to a client (usually a web browser). This lets us use PHP, Python, Ruby and other languages to handle dynamic web requests.

Nginx, then, can be used as a frontman for your application. Rather than handling the request, it dispatches dynamic requests off to a process and returns the result. This gives you the ability to unburden the loading of static assets (html, images, js, css) from your web application and even lets you install Nginx on a separate server.

Futhermore, and similar to NodeJS, Nginx is evented and runs asynchronously. This helps Nginx work with (relatively) very low memory usage, ultimately also helping your server serve more requests.

Let's see how this works with a few languages.

### Install Nginx

This is different per flavor of Linux you use. In general, their [installation docs](http://wiki.nginx.org/Install) are good for the more popular flavors (Debign/Ubuntu and RedHat/CentOS).

For Ubuntu, which I continue to use for both ease and popularity (and thus good support/Googleablity) the install process looks like this:

    # If you don't already have this:
    sudo apt-get install -y python-software-properties

    # Add repository of stable builds:
    sudo add-apt-repository -y ppa:nginx/stable

    # Update local repositories after adding nginx/stable
    sudo apt-get update

    # Install latest stable nginx
    sudo apt-get install -y nginx

    # Start nginx
    sudo service nginx start

### Configuring Nginx

Similar to Apache configuration, Nginx in Ubuntu uses the `/etc/nginx/sites-available` and `/etc/nginx/sites-enabled` directory convention. Configuration files in `sites-available` can be enabled by sym-linking them to the `sites-enabled` directory (and then reloaded Nginx's configuration).

These configuration files are analogous to Apache's Virtual Hosts. Let's create a basic configuration file to serve static site files. Create/edit `/etc/nginx/sites-available/example.conf`:

    server {
        listen 80;
        
        root /var/www;
        index index.html index.htm;
        
        # Will also reply to "localhost" results
        server_name example.com;
        
        # If root URI
        location / {
            try_files $uri $uri/;
        }
    }

This will instruct Nginx to listen on port 80 and pull files from the `/var/www` directory for requests made to the `example.com` domain. We also define the "index" documents to try (in order of precendence). Pretty simple!

> This sets up Nginx to reply to the domain `example.com`. You may want to set it to something like `ip-address.xip.io` so you can avoid editing your hosts file to make `example.com` work. For example, `192.168.33.10.xip.io`. 

Here's [another Nginx config file](https://gist.github.com/fideloper/9477321) with some extra options you may want to explore and use as well.

Once you have a configuration setup, you can symlink it to Nginx's `sites-enabled` directory and reload Nginx!

	# Symlink available site to enabled site
	$ sudo ln -s /etc/nginx/sites-available/example.conf /etc/nginx/sites-enabled/example.conf
	# Reload Nginx config
	$ sudo service nginx reload
	
Then you're site should be up and running! Be sure to check out the screencasts below for an example of this setup and some extra tricks.

Here's how to use Nginx with our favorite languages.

* with [PHP](http://fideloper.com/ubuntu-12-04-lemp-nginx-setup), or see the screencasts below!
* with [NodeJS](http://stackoverflow.com/questions/5009324/node-js-nginx-and-now)
* with [Python](http://michal.karzynski.pl/blog/2013/06/09/django-nginx-gunicorn-virtualenv-supervisor/) - **[Gunicorn docs](http://gunicorn.org/)**
* with [Ruby on Rails](https://coderwall.com/p/yz8cha), or [Rails with Capistrano](http://ariejan.net/2011/09/14/lighting-fast-zero-downtime-deployments-with-git-capistrano-nginx-and-unicorn/) and finally [nginx+unicorn](http://sirupsen.com/setting-up-unicorn-with-nginx/) - **[Unicorn docs](http://unicorn.bogomips.org/)**

---

## Nginx as a Load Balancer

Since you've seen how we can pass web requests off to another process (like we hand off web requests to php-fpm, unicorn or gunicorn), you may have realized that Nginx can also act as a load balancer. It can distribute web requests amongst group of other servers or processes.

Before you add a load  balancer to your stack, read up on [considerations you may need to make in your web application](http://fideloper.com/web-app-load-balancing). Some considerations outlined there:

* User Sessions can persist across connections to different servers (or don't need to)
* SSL considerations are met (what servers have the SSL certificate(s))
* User-uploaded files live in a central store rather than on the web server the user happened to connect to
* Your application is proxy-aware (Get's the correct client IP address, port and protocol)

After you've ensured your web application is setup for a distributed environment, you can then decide on a strategy for load balancing. Nginx offers these strategies:

* **Round Robin** - Nginx switches which server to fulfill the requewt in order they are defined
* **Least Connections** - Request is assigned to the server with the least connections (and presumably the lowest load)
* **Ip-Hash/Sticky Sessions** - The Client's IP address is hashed. Ther resulting hash is used to determine which server to send the request to. This also makes user sessions "sticky". Subsequent requests from a specific user always get routed to the same server. This is one way to get around the issue of user sessions behaving as expected in a distributed environment.
* **Weight** - With any of the above strategies, you can assign weights to a server. Heavier-weighted servers are more likely to be selected to server a request. This is good if you want to use a partiuclarly powerful server more, or perhaps to use a server with newer or experimental specs/software installed less.

### Configuration

A basic configuration is really simple. Let's say we have three node.js processes running, and we want to distribute requests amongst them. We can configure our Nginx like so:

	# Define your "upstream" servers - the
	# servers request will be sent to
    upstream app_example {
        least_conn;                 # Use Least Connections strategy
        server 127.0.0.1:9000;      # NodeJS Server 1
        server 127.0.0.1:9001;      # NodeJS Server 2
        server 127.0.0.1:9002;      # NodeJS Server 3
    }

	# Define the Nginx server
	# This will proxy any non-static directory
    server {
        listen 80;
        server_name example.com www.example.com;

        access_log /var/log/nginx/example.com-access.log;
        error_log  /var/log/nginx/example.com-error.log error;
        
        # Browser and robot always look for these
        # Turn off logging for them
        location = /favicon.ico { log_not_found off; access_log off; }
        location = /robots.txt  { log_not_found off; access_log off; }
        
        # Handle static files so they are not proxied to NodeJS
        # You may want to also hand these requests to other upstream 
        # servers, as you can define more than one!
        location ~ ^/(images/|img/|javascript/|js/|css/|stylesheets/|flash/|media/|static/|robots.txt|humans.txt|favicon.ico) {
          root /var/www;
        }
        
        # pass the request to the node.js server 
        # with some correct headers for proxy-awareness
        location / {
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Host $http_host;
            proxy_set_header X-NginX-Proxy true;

            proxy_pass http://app_example/;
            proxy_redirect off;

            # Handle Web Socket connections
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
    }

    # Let's do an SSL setup also
    server {
        listen 443;

        # You'll need to have your own certificate and key files
        # This is not something to blindly copy and paste
        ssl on;
        ssl_certificate     /etc/ssl/example.com/example.com.crt;
        ssl_certificate_key /etc/ssl/example.com/example.com.key;

        # ... the rest here would be just like above ...
    }

The above Nginx setup proxies requests to three local node processes which are setup to accept HTTP requests and respond to them.

**Note** that the configuration also checks for locations of **static** files. Rather than hand off static files for our Node processes to handle, Nginx handles the static requests itself.

If you needed to, you could also define *another* block of Upstream servers to handle static files. That way the Nginx server would purely be a load balancer.

If you want to see the test Node.js server's I used for this to follow along, they are as follows. I had the following in a `server.js` file:

    var http = require('http');

    function serve(ip, port)
    {
            http.createServer(function (req, res) {
                res.writeHead(200, {'Content-Type': 'text/plain'});
                res.end("There's no place like "+ip+":"+port+"\n");
            }).listen(port, ip);
            console.log('Server running at http://'+ip+':'+port+'/');
    }

    serve('127.0.0.1', 9000);
    serve('127.0.0.1', 9001);
    serve('127.0.0.1', 9002);

---

### Other Tricks

* Official docs on using [Nginx as load balancer](http://nginx.org/en/docs/http/load_balancing.html)
* Using Nginx as [image processor](http://leafo.net/posts/creating_an_image_server.html)  (and Hacker News [comments with more good info](https://news.ycombinator.com/item?id=6419064))
* Use [Nginx with Vaprobash](http://fideloper.github.io/Vaprobash/) for development

---


## Nginx Basics

Nginx basics and some information on using wildcard subdomains.

<iframe src="//player.vimeo.com/video/89161695" width="100%" height="517" style="width:100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

## Nginx with PHP-FPM

Using wildcard subdomains to use one server for your PHP projects.

<iframe src="//player.vimeo.com/video/89267501" width="100%" height="517" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
