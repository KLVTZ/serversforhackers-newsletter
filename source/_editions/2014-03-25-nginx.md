---
draft: true
title: Nginx Instead
topics: [Nginx as Frontman, Nginx Tricks]

---

## DONT ADD THIS IN: SPONSOR
Zack at z19r.com.

## Nginx as Frontman

> Apache is like Microsoft Word, it has a million options but you only need six. Nginx does those six things, and it does five of them 50 times faster than Apache. - [Chris Lea](http://wiki.nginx.org/WhyUseIt)

Nginx is a lightweight alternative to Apache. It does less things, but still covers 90% of the most common use cases.

When we run PHP with Apache, a typical setup is to use the PHP5 Module. This means PHP is essentially loaded and available on every request, even for static files and images! That creates some overhead for each web request.

Nginx, on the other hand, deals primarily with static files, prefering to hand-off other requests to another process. This way, Nginx doesn't have to load in PHP or know about Python or Ruby in order to handle a web request.

How, then, does Nginx hand-off a request to a dynamic application? As mentioned, it passes the request off to another process instead of handling the request itself. With PHP, for example, we would install PHP5-FPM alongside Nginx. Nginx would then hande-off any PHP request to PHP5-FPM to process and respond to. In fact, we can install any type of CGI/FastCGI process, or even another http server, to handle a request. These would then process the request and return the result for Nginx to ultimately return to a client (usually a web browser). This lets us use PHP, Python, Ruby and other languages to handle dynamic web requests.

Nginx, then, is actually a frontman for your application. Rather than handling the request, it dispatches dynamic requests off to a process and returns the result. This lets you even install Nginx on a server that's separate from your application.

Futhermore, and similar to NodeJS, Nginx is evented and runs asynchronously. This helps Nginx work with very low memory usage when its serving static files, ultimately also helping it serve more requests.

Let's see how this works with a few languages.

### Install Nginx

This is different per flavor of Linux you use. In general, their [installation docs](http://wiki.nginx.org/Install) are good for the more popular flavors (Debign/Ubuntu and RedHat/CentOS).

For Ubuntu, which I deem to be the most popular because I just said so right here and you just read it on the internet so it must be true, the install process looks like this:

    # If you don't already have this:
    sudo apt-get install -y python-software-properties

    # Add repository of stable builds:
    sudo add-apt-repository -y ppa:nginx/stable

    # Update local repositories after adding nginx/stable
    sudo apt-get update

    # Install latest stable nginx
    sudo apt-get install -y nginx

    # Start nginx
    sudo service nginx start

### Configuring Nginx

Similar to Apache, Nginx in Ubuntu often has a `/etc/nginx/sites-available` and `/etc/nginx/sites-enabled` directory. Configuration files in `sites-available` can be enabled by symlinking them to the `sites-enabled` directory (and then reloaded Nginx's configuration).

These are essentially just like Apache Virtual Hosts, altho they aren't called that in Nginx exactly. Create `/etc/nginx/sites-available/example.conf`:

    server {
        listen 80;
        
        root /var/www;
        index index.html index.htm;
        
        server_name example.com;
        
        # If root URI
        location / {
            try_files $uri $uri/;
        }
    }

This will instruct Nginx to listen on port 80 and pull files from the `/var/www` directory for requests made to the `example.com` domain. We also define the "index" documents to try (in order of precendence). Pretty simple!

Here's [another Nginx config file](https://gist.github.com/fideloper/9477321) with some extra options you may want to explore and use as well.

Once you have a configuration setup, you can symlink it to Nginx's `sites-enabled` directory and reload Nginx!

	# Symlink available site to enabled site
	$ sudo ln -s /etc/nginx/sites-available/example.conf /etc/nginx/sites-enabled/example.conf
	# Reload Nginx config
	$ sudo service nginx reload
	
Then you're site should be up and running! Be sure to check out the screencasts below for an example of this setup and some extra tricks.

Here's how to use Nginx with our favorite languages.

* with [PHP](http://fideloper.com/ubuntu-12-04-lemp-nginx-setup)
* with [NodeJS](http://stackoverflow.com/questions/5009324/node-js-nginx-and-now)
* with [Python](http://michal.karzynski.pl/blog/2013/06/09/django-nginx-gunicorn-virtualenv-supervisor/) - **[Gunicorn docs](http://gunicorn.org/)**
* with [Ruby on Rails](https://coderwall.com/p/yz8cha), or [Rails with Capistrano](http://ariejan.net/2011/09/14/lighting-fast-zero-downtime-deployments-with-git-capistrano-nginx-and-unicorn/) and finally [nginx+unicorn](http://sirupsen.com/setting-up-unicorn-with-nginx/) - **[Unicorn docs](http://unicorn.bogomips.org/)**

---

## Nginx as a Load Balancer

Since you've seen how we can pass web requests off to another process (like we hand off web requests to php-fpm, unicorn or gunicorn), you may have realized that Nginx can also act as a load balancer. It can distribute web requests amongst group of other servers or processes.

Before you add a load  balancer to your stack, read up on [considerations you may need to make in your web application](http://fideloper.com/web-app-load-balancing). Some considerations outlined there:

* User Sessions can persist across connections to different servers (or don't need to)
* SSL considerations are met (what servers have the SSL certificate(s))
* User-uploaded files live in a central store rather than on the web server the user happened to connect to
* Your application is proxy-aware

After you've ensured your web application is setup for a distributed environment, you can then decide on a strategy for load balancing. Nginx offers these strategies:

* **Round Robin** - Nginx switches which server to fulfill the requewt in order they are defined
* **Least Connections** - Request is assigned to the server with the least connections (and presumably the lowest load)
* **Ip-Hash/Sticky Sessions** - The Client's IP address is hashed. Ther resulting hash is used to determine which server to send the request to. This also makes user sessions "sticky". Subsequent requests from a specific user always get routed to the same server. This is one way to get around the issue of user sessions behaving as expected in a distributed environment.
* **Weight** - With any of the above strategies, you can assign weights to a server. Heavier-weighted servers are more likely to be selected to server a request. This is good if you want to use a partiuclarly powerful server more, or perhaps to use a server with newer or experimental specs/software installed less.

### Configuration

A basic configuration is really simple. Let's say we have three node.js processes running, and we want to distribute requests amongst them. We can configure our Nginx like so:

    upstream app_example {
        least_conn;                 # Use Least Connections strategy
        server 127.0.0.1:3000;      # NodeJS Server 1, happens to be local to this server
        server 127.0.0.1:3001;      # NodeJS Server 2, happens to be local to this server
        server 127.0.0.1:3002;      # NodeJS Server 3, happens to be local to this server
    }

    server {
        listen 80;
        server_name exammple.com www.example.com;

        access_log /var/log/nginx/example.com-access.log;
        error_log  /var/log/nginx/example.com-error.log;
        
        # Browser and robot always look for these
        # Turn off logging for them
        location = /favicon.ico { log_not_found off; access_log off; }
        location = /robots.txt  { log_not_found off; access_log off; }
        
        # pass the request to the node.js server 
        # with some correct headers for proxy-awareness
        location / {
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Host $http_host;
            proxy_set_header X-NginX-Proxy true;

            proxy_pass http://app_example/;
            proxy_redirect off;

            # Handle Web Socket connections
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
    }

    # Let's do an SSL also
    server {
        listen 443;

        # You'll need to have your own certificate and key files
        # This is not something to blindly copy and paste
        ssl on;
        ssl_certificate     /etc/ssl/example.com/example.com.crt;
        ssl_certificate_key /etc/ssl/example.com/example.com.key;

        server_name exammple.com www.example.com;

        access_log /var/log/nginx/example.com-access.log;
        error_log  /var/log/nginx/example.com-error.log;
        
        # Browser and robot always look for these
        # Turn off logging for them
        location = /favicon.ico { log_not_found off; access_log off; }
        location = /robots.txt  { log_not_found off; access_log off; }
        
        # pass the request to the node.js server 
        # with some correct headers for proxy-awareness
        location / {
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Host $http_host;
            proxy_set_header X-NginX-Proxy true;

            # Note: HTTP used here still. HTTPS is terminated
            # at Nginx in this example
            proxy_pass http://app_example/;
            proxy_redirect off;

            # Handle Web Socket connections
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
    }


---

### Other Tricks

* Official docs on using [Nginx as load balancer](http://nginx.org/en/docs/http/load_balancing.html)
* Using Nginx as [image processor](http://leafo.net/posts/creating_an_image_server.html)  (and Hacker News [comments with more good info](https://news.ycombinator.com/item?id=6419064))
* Use [Nginx with Vaprobash now](http://fideloper.github.io/Vaprobash/).

---


## Nginx Basics

Nginx basics and some information on using wildcard subdomains.

<iframe src="//player.vimeo.com/video/89161695" width="100%" height="517" style="width:100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

## Nginx with PHP-FPM

Using wildcard subdomains to use one server for your PHP projects.

<iframe src="//player.vimeo.com/video/89267501" width="100%" height="517" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
